<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Papers</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <style>
        /* Theme Variables */
        :root {
            --bg-color: #0a0a18;
            --text-color: #e8d5ff;
            --heading-color: #ff65d4;
            --link-color: #c576ff;
            --strong-color: #ff3980;
            --em-color: #a4c2ff;
            --card-bg: #10102a;
            --card-border: #252562;
            --accent-color: #8034eb;
            --selection-bg: rgb(174, 111, 255);
            --selection-color: #ffb6b6;
            --scrollbar-thumb: #444;
            --scrollbar-track: #111;
            --toc-bg: rgba(16, 16, 42, 0.95);
            --toc-hover: #1e1e4d;
            --toggle-bg: #222;
            --toggle-dot: #ff75ca;
            --insight-bg: rgba(128, 52, 235, 0.15);
            --progress-bar-color: linear-gradient(90deg, #8034eb, #ff3980);
            --code-bg: #111;
            --button-bg: #8034eb;
            --button-hover: #9c5aff;
            --tag-bg: rgba(128, 52, 235, 0.2);
            --bookmark-icon: #ff75ca;
        }

        /* Additional themes */
        [data-theme="light"] {
            --bg-color: #f5f5ff;
            --text-color: #333355;
            --heading-color: #7a0094;
            --link-color: #8800cc;
            --strong-color: #e30078;
            --em-color: #4b66b0;
            --card-bg: #ffffff;
            --card-border: #d0d0ff;
            --accent-color: #8034eb;
            --selection-bg: rgb(174, 111, 255);
            --selection-color: #ffffff;
            --scrollbar-thumb: #c5c5c5;
            --scrollbar-track: #f1f1f1;
            --toc-bg: rgba(255, 255, 255, 0.9);
            --toc-hover: #f0f0ff;
            --toggle-bg: #e0e0e0;
            --toggle-dot: #9c00c7;
            --insight-bg: rgba(163, 20, 220, 0.08);
            --progress-bar-color: linear-gradient(90deg, #8034eb, #d4006e);
            --code-bg: #f0f0ff;
            --button-bg: #8034eb;
            --button-hover: #9c5aff;
            --tag-bg: rgba(128, 52, 235, 0.15);
            --bookmark-icon: #d4006e;
        }

        [data-theme="cyberpunk"] {
            --bg-color: #0a0b1b;
            --text-color: #00ffea;
            --heading-color: #ff00a0;
            --link-color: #ffff00;
            --strong-color: #ff6600;
            --em-color: #00ccff;
            --card-bg: #131629;
            --card-border: #303076;
            --accent-color: #00ffea;
            --selection-bg: #ff00a0;
            --selection-color: #000000;
            --scrollbar-thumb: #ff00a0;
            --scrollbar-track: #131629;
            --toc-bg: rgba(19, 22, 41, 0.95);
            --toc-hover: #1e1e4d;
            --toggle-bg: #303076;
            --toggle-dot: #00ffea;
            --insight-bg: rgba(0, 255, 234, 0.1);
            --progress-bar-color: linear-gradient(90deg, #ff00a0, #00ffea);
            --code-bg: #0e0f1d;
            --button-bg: #ff00a0;
            --button-hover: #ff35b5;
            --tag-bg: rgba(0, 255, 234, 0.2);
            --bookmark-icon: #ffff00;
        }

        [data-theme="nature"] {
            --bg-color: #0c2818;
            --text-color: #c1e3c1;
            --heading-color: #54a759;
            --link-color: #89d089;
            --strong-color: #ee8434;
            --em-color: #47a697;
            --card-bg: #102d1c;
            --card-border: #1b4332;
            --accent-color: #54a759;
            --selection-bg: #54a759;
            --selection-color: #ffffff;
            --scrollbar-thumb: #54a759;
            --scrollbar-track: #102d1c;
            --toc-bg: rgba(16, 45, 28, 0.95);
            --toc-hover: #153823;
            --toggle-bg: #1b4332;
            --toggle-dot: #89d089;
            --insight-bg: rgba(84, 167, 89, 0.1);
            --progress-bar-color: linear-gradient(90deg, #54a759, #89d089);
            --code-bg: #0a2015;
            --button-bg: #54a759;
            --button-hover: #6ec273;
            --tag-bg: rgba(84, 167, 89, 0.2);
            --bookmark-icon: #ee8434;
        }

        * {
            box-sizing: border-box;
            transition: background-color 0.3s, color 0.3s, transform 0.3s, box-shadow 0.3s;
        }

        body {
            font-family: 'Segoe UI', system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
            margin: 0;
            padding: 0;
            background: var(--bg-color) url('data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMDAlIiBoZWlnaHQ9IjEwMCUiPjxkZWZzPjxwYXR0ZXJuIGlkPSJwYXR0ZXJuXzEiIHBhdHRlcm5Vbml0cz0idXNlclNwYWNlT25Vc2UiIHdpZHRoPSIxMCIgaGVpZ2h0PSIxMCIgcGF0dGVyblRyYW5zZm9ybT0icm90YXRlKDQ1KSI+PGxpbmUgeDE9IjAiIHkxPSIwIiB4Mj0iMCIgeTI9IjEwIiBzdHJva2U9IiMxNTE1MzUiIHN0cm9rZS13aWR0aD0iMSIvPjwvcGF0dGVybj48L2RlZnM+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgZmlsbD0idXJsKCNwYXR0ZXJuXzEpIiBvcGFjaXR5PSIwLjAzIi8+PC9zdmc+') repeat;
            color: var(--text-color);
            line-height: 1.7;
            font-size: 16px;
            scroll-behavior: smooth;
        }

        ::selection {
            background-color: var(--selection-bg);
            color: var(--selection-color);
        }

        ::-webkit-scrollbar {
            width: 12px;
            height: 12px;
        }

        ::-webkit-scrollbar-track {
            background: var(--scrollbar-track);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--scrollbar-thumb);
            border-radius: 6px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--link-color);
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: all 0.2s;
        }

        a:hover {
            text-decoration: underline;
            opacity: 0.9;
        }

        h1, h2 {
            font-weight: 600;
            margin-top: 1.5em;
            margin-bottom: 0.5em;
            line-height: 1.3;
            scroll-margin-top: 80px;
        }

        h1 {
            color: var(--heading-color);
            font-size: 1.8em;
            border-bottom: 1px solid var(--card-border);
            padding-bottom: 0.5em;
        }

        h2 {
            color: var(--strong-color);
            font-size: 1.5em;
        }

        ul {
            padding-left: 25px;
        }

        li {
            margin-bottom: 12px;
            position: relative;
        }
        
        .paper-list li {
            transition: transform 0.3s;
            padding-left: 5px;
            position: relative;
        }
        
        .paper-list li:hover {
            transform: translateX(5px);
        }

        .paper-list li a::before {
            content: '';
            position: absolute;
            left: -15px;
            top: 50%;
            width: 0;
            height: 2px;
            background-color: var(--link-color);
            transform: translateY(-50%);
            transition: width 0.3s ease;
        }

        .paper-list li:hover a::before {
            width: 12px;
        }

        .wrapper {
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            position: relative;
        }

        .page-content {
            padding: 20px;
            background-color: var(--bg-color);
            border-radius: 8px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2);
        }

        .paper-card {
            background: linear-gradient(135deg, var(--card-bg), rgba(16, 16, 42, 0.9));
            border-width: 1px;
            position: relative;
            overflow: hidden;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.15);
            backdrop-filter: blur(8px);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 20px 30px;
            margin: 30px 0;
            transition: transform 0.3s, box-shadow 0.3s;
        }

        .paper-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 5px;
            height: 100%;
            background: linear-gradient(to bottom, var(--heading-color), var(--accent-color));
        }

        .paper-card::after {
            content: '';
            position: absolute;
            top: 0;
            right: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(45deg, transparent 65%, rgba(255, 255, 255, 0.05) 75%, transparent 85%);
            pointer-events: none;
        }

        .paper-card:hover {
            transform: translateY(-8px) scale(1.01);
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.2);
        }

        .paper-meta {
            font-style: italic;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 10px;
        }

        .paper-link {
            display: inline-flex;
            align-items: center;
            margin-left: 10px;
            border: 1px solid var(--link-color);
            padding: 3px 8px;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.2s, color 0.2s;
        }

        .paper-link:hover {
            background-color: var(--link-color);
            color: var(--bg-color);
            text-decoration: none;
        }

        .paper-link i {
            margin-right: 5px;
        }

        p {
            line-height: 1.7;
            margin-bottom: 1em;
        }

        strong {
            color: var(--strong-color);
            font-weight: 600;
        }

        em {
            color: var(--em-color);
        }

        blockquote {
            border-left: 3px solid var(--accent-color);
            margin-left: 0;
            padding-left: 20px;
            font-style: italic;
            color: var(--em-color);
        }

        hr {
            border: 0;
            height: 1px;
            background: var(--card-border);
            margin: 40px 0;
        }

        img {
            max-width: 100%;
            border: 1px solid var(--card-border);
            border-radius: 8px;
            margin: 20px auto;
            display: block;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s;
        }

        img:hover {
            transform: scale(1.02);
        }

        code {
            background-color: var(--code-bg);
            padding: 2px 5px;
            border-radius: 3px;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 0.9em;
        }

        /* Header Styles */
        .header {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            padding: 15px 0;
            background: linear-gradient(90deg, var(--bg-color), rgba(10, 10, 24, 0.9));
            z-index: 1000;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(10px);
        }

        .header-content {
            max-width: 1000px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .site-title {
            font-size: 1.5em;
            font-weight: 700;
            color: var(--heading-color);
            margin: 0;
            position: relative;
            display: inline-flex;
            align-items: center;
            position: relative;
            overflow: hidden;
            display: inline-block;
        }

        .site-title i {
            margin-right: 10px;
            font-size: 0.9em;
        }

        .site-title:after {
            content: '';
            position: absolute;
            bottom: -5px;
            left: 0;
            width: 100%;
            height: 2px;
            background: linear-gradient(90deg, transparent, var(--accent-color), transparent);
            animation: shine 3s infinite;
        }

        @keyframes shine {
            0% { left: -100%; }
            100% { left: 100%; }
        }

        .header-controls {
            display: flex;
            gap: 15px;
            align-items: center;
        }

        /* Progress Bar */
        .progress-container {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 3px;
            z-index: 1010;
        }

        .progress-bar {
            height: 100%;
            background: var(--progress-bar-color);
            width: 0%;
            transition: width 0.2s;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: relative;
            width: 50px;
            height: 24px;
            background-color: var(--toggle-bg);
            border-radius: 12px;
            cursor: pointer;
        }

        .theme-toggle:before {
            content: '';
            position: absolute;
            width: 20px;
            height: 20px;
            background-color: var(--toggle-dot);
            border-radius: 50%;
            top: 2px;
            left: 2px;
            transition: transform 0.3s;
        }

        [data-theme="light"] .theme-toggle:before {
            transform: translateX(26px);
        }

        .theme-icons {
            position: absolute;
            display: flex;
            justify-content: space-between;
            width: 36px;
            left: 7px;
            top: 3px;
            color: white;
            font-size: 12px;
        }

        /* TOC Styles */
        .toc-toggle {
            border: none;
            background: var(--button-bg);
            color: white;
            border-radius: 4px;
            padding: 6px 12px;
            font-size: 14px;
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 5px;
            transition: background-color 0.2s;
        }

        .toc-toggle:hover {
            background-color: var(--button-hover);
        }

        .toc-container {
            position: fixed;
            top: 70px;
            bottom: 20px;
            right: 20px;
            width: 300px;
            background-color: var(--toc-bg);
            border-radius: 8px;
            overflow-y: auto;
            padding: 20px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
            transform: translateX(320px);
            transition: transform 0.3s;
            z-index: 990;
            backdrop-filter: blur(5px);
        }

        .toc-container.active {
            transform: translateX(0);
        }

        .toc-title {
            margin-top: 0;
            padding-bottom: 10px;
            border-bottom: 1px solid var(--card-border);
            font-size: 16px;
            color: var(--heading-color);
        }

        .toc-list {
            padding-left: 0;
            list-style-type: none;
            max-height: calc(100vh - 210px);
            overflow-y: auto;
        }

        .toc-list li {
            margin-bottom: 8px;
            padding: 5px;
            border-radius: 4px;
            transition: background-color 0.2s;
            position: relative;
        }

        .toc-list li:hover {
            background-color: var(--toc-hover);
        }

        .toc-list li.active {
            background-color: var(--toc-hover);
            border-left: 3px solid var(--accent-color);
            padding-left: 10px;
        }

        .toc-list a {
            display: block;
            width: 100%;
            text-overflow: ellipsis;
            overflow: hidden;
            white-space: nowrap;
            padding-right: 20px;
        }

        .toc-list .paper-year {
            color: var(--em-color);
            font-size: 0.8em;
            margin-right: 5px;
        }

        .search-container {
            position: relative;
            margin-top: 15px;
            margin-bottom: 20px;
        }

        .search-input {
            width: 100%;
            padding: 10px 15px;
            padding-right: 40px;
            border-radius: 20px;
            border: 1px solid var(--card-border);
            background-color: var(--card-bg);
            color: var(--text-color);
            font-size: 14px;
            transition: border-color 0.3s, box-shadow 0.3s;
        }

        .search-input:focus {
            outline: none;
            border-color: var(--link-color);
            box-shadow: 0 0 0 2px rgba(128, 52, 235, 0.2);
        }

        .search-icon {
            position: absolute;
            right: 15px;
            top: 50%;
            transform: translateY(-50%);
            color: var(--text-color);
            opacity: 0.7;
        }

        .main-content {
            padding-top: 70px;
        }

        /* Insights styling */
        .insight-header {
            display: flex;
            align-items: center;
            gap: 10px;
            margin-top: 25px;
            margin-bottom: 15px;
            color: var(--accent-color);
            padding: 10px 15px;
            background: linear-gradient(90deg, var(--insight-bg), transparent);
            border-radius: 6px;
            border-left: 3px solid var(--accent-color);
        }

        .insight-header i {
            font-size: 1.2em;
        }

        .insight-header strong {
            color: var(--accent-color);
        }

        .insight-content {
            margin-left: 15px;
            padding-left: 15px;
            border-left: 1px dashed var(--card-border);
        }

        /* Button Styles */
        .custom-button {
            background-color: var(--button-bg);
            color: white;
            border: none;
            border-radius: 4px;
            padding: 8px 16px;
            cursor: pointer;
            font-size: 14px;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: background-color 0.2s;
        }

        .custom-button:hover {
            background-color: var(--button-hover);
        }

        .custom-button i {
            font-size: 16px;
        }

        /* Tag styles */
        .tags-container {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin: 20px 0;
        }

        .tag {
            background: linear-gradient(135deg, var(--tag-bg), rgba(128, 52, 235, 0.1));
            border: 1px solid rgba(128, 52, 235, 0.2);
            padding: 5px 12px;
            border-radius: 25px;
            font-size: 0.85em;
            cursor: pointer;
            transition: background-color 0.2s, transform 0.2s;
            display: inline-flex;
            align-items: center;
            gap: 5px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }

        .tag:hover {
            transform: translateY(-2px);
            background-color: var(--tag-bg);
            opacity: 0.9;
        }

        .tag.active {
            background: linear-gradient(135deg, var(--accent-color), rgba(128, 52, 235, 0.8));
            color: var(--bg-color);
            box-shadow: 0 2px 15px rgba(128, 52, 235, 0.4);
        }

        /* Bookmark icon */
        .bookmark-icon {
            position: absolute;
            top: 15px;
            right: 15px;
            font-size: 1.3em;
            color: var(--card-border);
            cursor: pointer;
            transition: all 0.3s;
            z-index: 10;
        }

        .bookmark-icon:hover {
            color: var(--bookmark-icon);
            transform: scale(1.3) rotate(10deg);
            text-shadow: 0 0 10px rgba(255, 117, 202, 0.5);
        }

        .bookmark-icon.active {
            color: var(--bookmark-icon);
            animation: pulse-bookmark 2s infinite;
        }

        @keyframes pulse-bookmark {
            0% { transform: scale(1); }
            50% { transform: scale(1.15); }
            100% { transform: scale(1); }
        }

        /* Theme selector */
        .theme-selector {
            position: fixed;
            bottom: 80px;
            right: 20px;
            background-color: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 8px;
            padding: 10px;
            display: flex;
            flex-direction: column;
            gap: 10px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
            z-index: 980;
            transform: translateX(100px);
            opacity: 0;
            visibility: hidden;
            transition: transform 0.3s, opacity 0.3s, visibility 0.3s;
        }

        .theme-selector.active {
            transform: translateX(0);
            opacity: 1;
            visibility: visible;
        }

        .theme-option {
            width: 30px;
            height: 30px;
            border-radius: 50%;
            cursor: pointer;
            transition: transform 0.2s;
            position: relative;
            border: 2px solid transparent;
        }

        .theme-option:hover {
            transform: scale(1.1);
        }

        .theme-option.active {
            border-color: var(--accent-color);
        }

        .theme-dark {
            background: linear-gradient(to bottom right, #050505, #350035);
        }

        .theme-light {
            background: linear-gradient(to bottom right, #f8f5ff, #e0e0ff);
        }

        .theme-cyberpunk {
            background: linear-gradient(to bottom right, #0a0b1b, #ff00a0);
        }

        .theme-nature {
            background: linear-gradient(to bottom right, #0c2818, #54a759);
        }

        .theme-palette-btn {
            position: fixed;
            bottom: 20px;
            right: 80px;
            background-color: var(--card-bg);
            color: var(--accent-color);
            border: 1px solid var(--card-border);
            border-radius: 50%;
            width: 50px;
            height: 50px;
            display: flex;
            justify-content: center;
            align-items: center;
            font-size: 20px;
            cursor: pointer;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            z-index: 980;
            transition: transform 0.3s, background-color 0.3s;
        }

        .theme-palette-btn:hover {
            transform: scale(1.1);
        }

        .theme-palette-btn.active {
            background-color: var(--accent-color);
            color: var(--bg-color);
        }

        /* Scroll top button */
        #scrollTopBtn {
            position: fixed;
            bottom: 20px;
            right: 20px;
            background-color: var(--accent-color);
            color: white;
            border: none;
            border-radius: 50%;
            width: 50px;
            height: 50px;
            font-size: 24px;
            cursor: pointer;
            display: none;
            justify-content: center;
            align-items: center;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
            z-index: 980;
            transition: background-color 0.3s, transform 0.3s;
        }

        #scrollTopBtn:hover {
            background-color: var(--button-hover);
            transform: translateY(-5px);
        }

        /* Animation for bookmarked papers */
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }

        .paper-card.bookmarked {
            animation: border-glow 3s infinite, float 6s infinite;
            border-color: var(--bookmark-icon);
            border-width: 2px;
        }

        @keyframes border-glow {
            0% { border-color: var(--card-border); }
            50% { border-color: var(--accent-color); }
            100% { border-color: var(--card-border); }
        }

        @keyframes float {
            0% { transform: translateY(0px); }
            50% { transform: translateY(-10px); }
            100% { transform: translateY(0px); }
        }

        /* Media queries */
        @media (max-width: 1280px) {
            .toc-container {
                width: 280px;
            }
        }

        @media (max-width: 900px) {
            .wrapper {
                padding: 10px;
            }

            .header-content {
                padding: 0 15px;
            }

            .toc-container {
                right: 0;
                top: 0;
                bottom: 0;
                border-radius: 0;
                width: 280px;
                z-index: 1001;
            }

            .overlay {
                position: fixed;
                top: 0;
                left: 0;
                right: 0;
                bottom: 0;
                background-color: rgba(0, 0, 0, 0.5);
                z-index: 1000;
                display: none;
            }

            .overlay.active {
                display: block;
            }

            .theme-palette-btn {
                bottom: 80px;
                right: 20px;
            }

            .theme-selector {
                bottom: 140px;
            }
        }

        @media (max-width: 600px) {
            body {
                font-size: 15px;
            }

            .paper-card {
                padding: 15px 20px;
            }

            .header-content {
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }

            .header-controls {
                width: 100%;
                justify-content: flex-end;
            }

            .paper-meta {
                flex-direction: column;
                align-items: flex-start;
                gap: 5px;
            }

            .paper-link {
                margin-left: 0;
            }

            .tags-container {
                justify-content: center;
            }
        }
    </style>
</head>
<body>
    <!-- Reading Progress Bar -->
    <div class="progress-container">
        <div class="progress-bar" id="readingProgress"></div>
    </div>

    <header class="header">
        <div class="header-content">
            <h1 class="site-title"><i class="fas fa-brain"></i> Notes</h1>
            <div class="header-controls">
                <div class="theme-toggle" id="themeToggle">
                    <div class="theme-icons">
                        <i class="fas fa-moon"></i>
                        <i class="fas fa-sun"></i>
                    </div>
                </div>
                <button class="toc-toggle" id="tocToggle">
                    <i class="fas fa-list"></i> Contents
                </button>
            </div>
        </div>
    </header>

    <div class="overlay" id="overlay"></div>

    <div class="toc-container" id="tocContainer">
        <h3 class="toc-title">Table of Contents</h3>
        <div class="search-container">
            <input type="text" class="search-input" id="searchInput" placeholder="Search papers...">
            <i class="fas fa-search search-icon"></i>
        </div>
        <ul class="toc-list" id="tocList">
            <!-- Will be populated by JavaScript -->
        </ul>
    </div>

    <main class="page-content main-content" aria-label="Content">
        <div class="wrapper">
            <article class="post h-entry">
                <div>
                    <h1 id="ai-papers-collection">AI Papers Collection</h1>
                    <p>I've collected the papers here, along with some key takeaways from each. They are in chronological order to make it easier to see how they build on each other. Most are papers, but a few are books, blog posts, course notes, etc. In any case, it's a lot of reading! I've highlighted a few that I think are particularly worth a look.</p>
                    
                    <div class="search-container">
                        <input type="text" class="search-input" id="mainSearchInput" placeholder="Search papers in collection...">
                        <i class="fas fa-search search-icon"></i>
                    </div>

                    <!-- Category/Tag Filtering -->
                    <div class="tags-container">
                        <div class="tag active" data-tag="all"><i class="fas fa-tag"></i> All Papers</div>
                        <div class="tag" data-tag="deep-learning"><i class="fas fa-network-wired"></i> Deep Learning</div>
                        <div class="tag" data-tag="nlp"><i class="fas fa-language"></i> NLP</div>
                        <div class="tag" data-tag="cv"><i class="fas fa-image"></i> Computer Vision</div>
                        <div class="tag" data-tag="theory"><i class="fas fa-calculator"></i> Theory</div>
                        <div class="tag" data-tag="bookmarked"><i class="fas fa-bookmark"></i> Bookmarked</div>
                    </div>
                    
                    <ul class="paper-list" id="paperList">
                        <li data-year="1993" data-tags="theory">
                            <a href="#keeping-neural-networks-simple-by-minimizing-the-description-length-of-the-weights">1993 - Keeping Neural Networks Simple By Minimizing the Description Length of the Weights</a>
                        </li>
                        <li data-year="2004" data-tags="theory">
                            <a href="#a-tutorial-introduction-to-the-minimum-description-length-principle">2004 - A Tutorial Introduction to the Minimum Description Length Principle</a>
                        </li>
                        <li data-year="2008" data-tags="theory">
                            <a href="#machine-super-intelligence">2008 - Machine Super Intelligence</a>
                        </li>
                        <li data-year="2011" data-tags="theory">
                            <a href="#the-first-law-of-complexodynamics">2011 - The First Law of Complexodynamics</a>
                        </li>
                        <li data-year="2012" data-tags="deep-learning cv">
                            <a href="#imagenet-classification-with-deep-convolutional-neural-networks">
                                <strong>2012 - ImageNet Classification with Deep Convolutional Neural Networks</strong>
                            </a>
                        </li>
                        <li data-year="2014" data-tags="deep-learning theory">
                            <strong>
                                <a href="#neural-turing-machines">2014 - Neural Turing Machines</a>
                            </strong>
                        </li>
                        <li data-year="2014" data-tags="theory">
                            <a href="#quantifying-the-rise-and-fall-of-complexity-in-closed-systems">2014 - Quantifying the Rise and Fall of Complexity in Closed Systems</a>
                        </li>
                        <li data-year="2015" data-tags="deep-learning cv">
                            <a href="#deep-residual-learning-for-image-recognition">2015 - Deep Residual Learning for Image Recognition</a>
                        </li>
                        <li data-year="2015" data-tags="deep-learning nlp">
                            <a href="#neural-machine-translation-by-jointly-learning-to-align-and-translate">
                                <strong>2015 - Neural Machine Translation by Jointly Learning to Align and Translate</strong>
                            </a>
                        </li>
                        <li data-year="2015" data-tags="deep-learning nlp">
                            <a href="#pointer-networks">2015 - Pointer Networks</a>
                        </li>
                        <li data-year="2015" data-tags="deep-learning nlp">
                            <a href="#recurrent-neural-network-regularization">2015 - Recurrent Neural Network Regularization</a>
                        </li>
                        <li data-year="2015" data-tags="deep-learning nlp">
                            <a href="#the-unreasonable-effectiveness-of-recurrent-neural-networks">
                                <strong>2015 - The Unreasonable Effectiveness of Recurrent Neural Networks</strong>
                            </a>
                        </li>
                        <li data-year="2015" data-tags="deep-learning nlp">
                            <a href="#understanding-lstm-networks">
                                <strong>2015 - Understanding LSTM Networks</strong>
                            </a>
                        </li>
                        <li data-year="2016" data-tags="deep-learning nlp">
                            <a href="#deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin">2016 - Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</a>
                        </li>
                        <li data-year="2016" data-tags="deep-learning cv">
                            <a href="#identity-mappings-in-deep-residual-networks">2016 - Identity Mappings in Deep Residual Networks</a>
                        </li>
                        <li data-year="2016" data-tags="deep-learning cv">
                            <a href="#multi-scale-context-aggregation-by-dilated-convolutions">2016 - Multi-Scale Context Aggregation by Dilated Convolutions</a>
                        </li>
                        <li data-year="2016" data-tags="deep-learning nlp">
                            <a href="#order-matters-sequence-to-sequence-for-sets">2016 - Order Matters: Sequence to sequence for sets</a>
                        </li>
                        <li data-year="2016" data-tags="deep-learning cv">
                            <a href="#variational-lossy-autoencoder">2016 - Variational Lossy Autoencoder</a>
                        </li>
                        <li data-year="2017" data-tags="deep-learning cv">
                            <a href="#a-simple-neural-network-module-for-relational-reasoning">2017 - A Simple Neural Network Module for Relational Reasoning</a>
                        </li>
                        <li data-year="2017" data-tags="deep-learning nlp">
                            <strong>
                                <a href="#attention-is-all-you-need">2017 - Attention is All You Need</a>
                            </strong>
                        </li>
                        <li data-year="2017" data-tags="theory">
                            <a href="#kolmogorov-complexity-and-algorithmic-randomness">2017 - Kolmogorov Complexity and Algorithmic Randomness</a>
                        </li>
                        <li data-year="2017" data-tags="deep-learning">
                            <a href="#neural-message-passing-for-quantum-chemistry">2017 - Neural Message Passing for Quantum Chemistry</a>
                        </li>
                        <li data-year="2018" data-tags="deep-learning nlp">
                            <a href="#relational-recurrent-neural-networks">2018 - Relational Recurrent Neural Networks</a>
                        </li>
                        <li data-year="2019" data-tags="deep-learning">
                            <a href="#gpipe-efficient-training-of-giant-neural-networks-using-pipeline-parallelism">2019 - GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a>
                        </li>
                        <li data-year="2020" data-tags="deep-learning nlp">
                            <a href="#scaling-laws-for-neural-language-models">
                                <strong>2020 - Scaling Laws for Neural Language Models</strong>
                            </a>
                        </li>
                        <li data-year="2024" data-tags="deep-learning cv">
                            <a href="#cs231n-convolutional-neural-networks-for-visual-recognition">2024 - CS231n Convolutional Neural Networks for Visual Recognition</a>
                        </li>
                    </ul>
                    
                    <!-- Paper sections start -->
                    <div class="paper-card" id="keeping-neural-networks-simple-by-minimizing-the-description-length-of-the-weights" data-year="1993" data-tags="theory">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="keeping-neural-networks-simple-by-minimizing-the-description-length-of-the-weights"></i>
                        <h1>Keeping Neural Networks Simple by Minimizing the Description Length of the Weights</h1>
                        <div class="paper-meta">
                            <span><em>Hinton, Geoffrey E., and DrewVan Camp, 1993</em></span>
                            <a class="paper-link" target="_blank" href="https://www.cs.toronto.edu/~fritz/absps/colt93.pdf">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>"Supervised neural networkler, weightlerindeki bilgi miktarı training caselerinin output vektörlerindeki bilgiden çok daha az olduğunda iyi genelleme yaparlar. Bu nedenle öğrenme sırasında, weightlerin içerdiği bilgi miktarını cezalandırarak weightleri basit tutmak önemlidir. Bir weight'teki bilgi miktarı, Gaussian noise eklenerek kontrol edilebilir ve gürültü seviyesi, networkteki beklenen kare hatası ile weightlerdeki bilgi miktarı arasındaki dengeyi optimize etmek için öğrenme sırasında ayarlanabilir. Non-linear hidden unitler içeren bir networkte beklenen kare hatanın ve gürültülü weightlerdeki bilgi miktarının türevlerini hesaplama yöntemini açıklıyoruz. Output unitler linear olduğunda, türevler zaman alıcı Monte Carlo simülasyonları olmadan verimli bir şekilde hesaplanabilir. Bir neural networkün weightlerini iletmek için gereken bilgi miktarını minimize etme fikri, weightleri kodlamak için bir dizi ilginç şema ortaya çıkarır."</em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <p>
                            Minimum Description Length Principle, bazı verilerin en iyi modelinin, model açıklamasının uzunluğu ile bu model kullanılarak kodlanmış verilerin uzunluğunun toplamını minimize eden model olduğunu belirtir.
                        </p>
                        <ul>
                            <li>
                                MDL prensibini neural networklere uygulayarak model karmaşıklığını kontrol eder ve overfittingi önler.
                            </li>
                            <li>
                                Weight-sharing kullanarak weightlerin description lengthini minimize eder: Aynı weight, ağdaki birden fazla bağlantıya uygulanarak, serbest parametre sayısını ve dolayısıyla model karmaşıklığını azaltır.
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="a-tutorial-introduction-to-the-minimum-description-length-principle" data-year="2004" data-tags="theory">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="a-tutorial-introduction-to-the-minimum-description-length-principle"></i>
                        <h1>A Tutorial Introduction to the Minimum Description Length Principle</h1>
                        <div class="paper-meta">
                            <span><em>Peter Grünwald, 2004</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/math/0406077">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <p>
                            <a href="https://en.wikipedia.org/wiki/Minimum_description_length">Minimum Description Length Principle</a> ve bunun model seçimine uygulamalarına açıkça yazılmış bir giriş.
                        </p>
                        <ul>
                            <li>
                                <strong>Veri sıkıştırma olarak öğrenme</strong>
                                Verilerdeki her düzenlilik, bu verileri sıkıştırmak için kullanılabilir ve öğrenme, bu düzenlilikleri bulmakla eşdeğer tutulabilir.
                            </li>
                            <li>
                                <strong>Model seçimi olarak MDL</strong>
                                En iyi model, modelin açıklamasının uzunluğu ile bu model kullanılarak kodlanmış verilerin uzunluğunun toplamını minimize eden modeldir.
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="machine-super-intelligence" data-year="2008" data-tags="theory">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="machine-super-intelligence"></i>
                        <h1>Machine Super Intelligence</h1>
                        <div class="paper-meta">
                            <span><em>Legg, Shane, 2008</em></span>
                            <a class="paper-link" target="_blank" href="https://www.vetta.org/documents/Machine_Super_Intelligence.pdf">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>Bu tez, bir agent'ın zeki olmasının ne anlama geldiğini tanımlamayı amaçlar. Zekânın resmi olmayan tanımlarından başlayarak, zekâyı matematiksel olarak genel, güçlü ve zarif bir şekilde tanımlar. Bu, bilinmeyen bir ortamla etkileşime girerek beklenen ödülünü maksimize eden optimal bir ajan kavramına yol açar. Pratik bir teori olmasa da – tanımları hesaplanabilir değildir – gerçek, optimal olmayan ajanları anlamak için teorik bir çerçeve sağlayabilir.</p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>"Zekâ, bir agent'ın çok çeşitli ortamlarda hedeflere ulaşma yeteneğini ölçer."</li>
                            <li>
                                Bir <strong>ortamın complexity</strong>'si, ortamın davranışını üreten en kısa programın uzunluğudur (<a target="_blank" href="https://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogorov complexity</a>
                                ).
                            </li>
                            <li>
                                Bir ortamın öncül olasılığı, karmaşıklığı ile üstel olarak azalır (<a target="_blank" href="https://en.wikipedia.org/wiki/Algorithmic_probability">Algorithmic probability</a>
                                ). Bu, Occam'ın usturasını resmileştirir.
                            </li>
                            <li>
                                Bir agent'ın <strong>Universal Intelligence</strong>'ı, ödüllerinin beklenen değeridir.
                            </li>
                            <li>Mükemmel veya "universal agents", herhangi bir hesaplanabilir ortamda beklenen ödüllerini maksimize eder. Hesaplanabilir olmasa da (çünkü Kolmogorov complexity hesaplanamaz), hesaplanabilir ajanların zekâsı üzerinde teorik bir üst sınır sağlar.</li>
                        </ul>
                    </div>

                    <div class="paper-card" id="the-first-law-of-complexodynamics" data-year="2011" data-tags="theory">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="the-first-law-of-complexodynamics"></i>
                        <h1>The First Law of Complexodynamics</h1>
                        <div class="paper-meta">
                            <span><em>Scott Aaronson, 2011.</em></span>
                            <a class="paper-link" target="_blank" href="https://scottaaronson.blog/?p=762">
                                <i class="fas fa-blog"></i> Blog Post
                            </a>
                        </div>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <p>
                            Teorik bilgisayar bilimcisi <a target="_blank" href="https://www.scottaaronson.com/">Scott Aaronson</a>
                            (complexity theory, quantum computing), "complex systems" ile ne kastettiğimizi belirlemeye çalışır ve karmaşıklığı titiz bir şekilde tanımlamanın ne anlama geleceği hakkında tahminlerde bulunur. Entropi ve (kaynak-sınırlı) Kolmogorov complexity gibi kavramları bir araya getirir. Burada bazı ilginç sorular var, ancak pratik çıkarımlar arıyorsanız bunu atlayabilirsiniz.
                        </p>
                    </div>

                    <div class="paper-card" id="imagenet-classification-with-deep-convolutional-neural-networks" data-year="2012" data-tags="deep-learning cv">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="imagenet-classification-with-deep-convolutional-neural-networks"></i>
                        <h1>ImageNet Classification with Deep Convolutional Neural Networks</h1>
                        <div class="paper-meta">
                            <span><em>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton, 2012</em></span>
                            <a class="paper-link" target="_blank" href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>
                                "Biz 1.2 milyon yüksek çözünürlüklü görüntüyü ImageNet LSVRC-2010 yarışmasında 1000 farklı sınıfa ayırmak için <strong>büyük, derin bir convolutional neural network</strong> eğittik. Test verileri üzerinde, önceki state-of-the-art'a göre önemli ölçüde daha iyi olan %37.5 ve %17.0 top-1 ve top-5 hata oranları elde ettik. <strong>60 milyon parametre ve 650.000 nöron</strong> içeren neural network, <strong>beş convolutional layer</strong>'dan oluşmaktadır, bunların bazılarını max-pooling layer'lar takip etmekte ve son olarak 1000 yönlü softmax ile <strong>üç fully-connected layer</strong> bulunmaktadır. Eğitimi hızlandırmak için, doymayan nöronlar ve konvolüsyon işleminin <strong>çok verimli bir GPU implementasyonu</strong> kullandık. Fully-connected layer'larda overfitting'i azaltmak için son zamanlarda geliştirilen ve <strong>çok etkili olduğu kanıtlanan "dropout" adlı bir regularization metodu</strong> kullandık. Ayrıca bu modelin bir varyasyonunu ILSVRC-2012 yarışmasına soktuk ve ikinci en iyi girişin elde ettiği %26.2'ye kıyasla %15.3'lük kazanan bir top-5 test hata oranı elde ettik."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                <strong>AlexNet, bir deep convolutional neural network</strong> tanıttı ve image classification'da benzeri görülmemiş bir performans elde etti. AlexNet, 2012'de ImageNet Large Scale Visual Recognition Challenge'ı önemli bir farkla kazanarak, deep learning'i bilgisayarla görü araştırmalarının ön saflarına taşıdı.
                            </li>
                            <li>
                                <strong>Derin Mimari</strong>
                                AlexNet'in derinliği – beş convolutional ve üç fully connected layer – ağın hiyerarşik özellik temsilleri öğrenmesine olanak tanıdı ve daha sığ mimarilerle mümkün olmayan verilerdeki karmaşık örüntüleri yakaladı.
                            </li>
                            <li>
                                <strong>GPU'larla Eğitim</strong>
                                Eğitimi hızlandırmak için GPU'lar, paralel hesaplama ve ReLU aktivasyon fonksiyonları kullanıldı.
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="neural-turing-machines" data-year="2014" data-tags="deep-learning theory">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="neural-turing-machines"></i>
                        <h1>Neural Turing Machines</h1>
                        <div class="paper-meta">
                            <span><em>Graves, Alex, Greg Wayne, and Ivo Danihelka, 2014</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1410.5401">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>
                                "Neural network'lerin yeteneklerini, <strong>attentional process'ler</strong> aracılığıyla etkileşime girebilecekleri <strong>harici bellek kaynaklarına</strong> bağlayarak genişletiyoruz. Birleştirilmiş sistem, bir Turing Makinesi veya Von Neumann mimarisine benzer, ancak <strong>uçtan uca diferansiyellenebilir</strong> olup, <strong>gradient descent ile verimli bir şekilde eğitilebilmesine</strong> olanak tanır. İlk sonuçlar, Neural Turing Machine'lerin kopyalama, sıralama ve ilişkisel hatırlama gibi basit algoritmaları girdi ve çıktı örneklerinden çıkarabildiğini göstermektedir."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                Neural Turing Machine'ler, kendi programlamalarını öğrenen <strong>tamamen diferansiyellenebilir bilgisayarlardır</strong>. Diferansiyellenebilirlik, NTM'leri gradient descent ile verimli bir şekilde eğitmenin anahtarıdır.
                            </li>
                            <li>Bir NN'i harici bellek ile güçlendirmek, klasik RNN'lerin önemli bir sınırlamasını ele alır: bilgiyi uzun süre saklama yetersizliği.</li>
                            <li>
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/130385.130432">Daha önceki çalışmalar</a>, <strong>RNN'lerin Turing Complete</strong> olduğunu, yani prensipte örneklerden algoritmaları öğrenme kabiliyetine sahip olduklarını gösterdi, ancak bunu gerçekte nasıl yapacaklarını göstermedi.
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="quantifying-the-rise-and-fall-of-complexity-in-closed-systems" data-year="2014" data-tags="theory">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="quantifying-the-rise-and-fall-of-complexity-in-closed-systems"></i>
                        <h1>Quantifying the Rise and Fall of Complexity in Closed Systems</h1>
                        <div class="paper-meta">
                            <span><em>Aaronson, Carroll, Ouelette, 2014</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1405.6903">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>
                                "<strong>Entropinin monoton olarak artmasının aksine, kapalı sistemlerin "karmaşıklığı" veya "ilginçliği" sezgisel olarak önce artıyor ve sonra denge durumuna yaklaştıkça azalıyor görünmektedir</strong>. Örneğin, evrenimiz Büyük Patlama'da karmaşık yapılardan yoksundu ve kara delikler buharlaştıktan ve parçacıklar dağıldıktan sonra da bunlardan yoksun olacak. Bu makale, bu modeli nicelendirmek için ilk girişimde bulunuyor. Model sistem olarak, iki sıvının ("kahve" ve "krema") karışımını simüle eden basit, iki boyutlu bir hücresel otomat kullanıyoruz. <strong>Makul bir karmaşıklık ölçüsü, otomatın durumunun kaba taneli bir yaklaşımının Kolmogorov karmaşıklığıdır; buna "görünür karmaşıklık" diyoruz</strong>. Bu karmaşıklık ölçüsünü inceliyoruz ve sıvı parçacıkları etkileşime girmediğinde karmaşıklığın asla büyük olmadığını analitik olarak gösteriyoruz. Buna karşılık, parçacıklar etkileşime girdiğinde, karmaşıklığın "kahve fincanının" yatay boyutuna benzer bir maksimuma ulaştığına dair sayısal kanıtlar sunuyoruz. Bu davranışı analitik olarak kanıtlama problemini ortaya koyuyoruz."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <p>
                            Bu makale, <a target="_blank" href="#the-first-law-of-complexodynamics">The First Law of Complexodynamics</a> makalesindeki fikirleri devam ettiriyor. Bence bunu güvenle atlayabilirsiniz.
                        </p>
                        <ul>
                            <li>
                                Bir n-bit dizinin entropisi genellikle <strong>Kolmogorov complexity ile tanımlanır, yani diziyi üreten en kısa programın uzunluğu</strong> ile.
                            </li>
                            <li>
                                <strong>Kolmogorov complexity'nin hesaplanamaz olduğu iyi bilinir</strong> (halting problem ile eşdeğer), ancak belki yaklaşık olarak hesaplanabilir? <em>"Kolmogorov complexity ve sophistication, entropi ve karmaşıklık fikirlerimizi modellemek için yararlı teorik kavramlar olsa da, ikisi de hesaplanamaz olduğundan sayısal simülasyonlarda doğrudan uygulanamaz."</em>
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="deep-residual-learning-for-image-recognition" data-year="2015" data-tags="deep-learning cv">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="deep-residual-learning-for-image-recognition"></i>
                        <h1>Deep Residual Learning for Image Recognition</h1>
                        <div class="paper-meta">
                            <span><em>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, 2015</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1512.03385">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>
                                "Daha derin neural network'leri eğitmek daha zordur. <strong>Daha önce kullanılanlardan önemli ölçüde daha derin olan ağların</strong> eğitimini kolaylaştırmak için bir residual learning framework sunuyoruz. Layer'ları, referanssız fonksiyonlar öğrenmek yerine, layer girişlerine referansla <strong>residual function'ları öğrenmek üzere açıkça yeniden formüle ediyoruz</strong>. Bu residual network'lerin optimize edilmesinin daha kolay olduğunu ve önemli ölçüde artan derinlikten doğruluk kazanabileceğini gösteren kapsamlı ampirik kanıtlar sunuyoruz. ImageNet veri setinde, 152 layer derinliğe kadar olan residual network'leri değerlendiriyoruz - VGG ağlarından 8 kat daha derin ama yine de daha düşük karmaşıklığa sahip. Bu residual network'lerin bir ensemble'ı, ImageNet test setinde %3.57 hata elde ediyor. Bu sonuç, <strong>ILSVRC 2015 classification görevinde 1. sırayı kazandı</strong>. Ayrıca CIFAR-10 üzerinde 100 ve 1000 layer'lı analizler de sunuyoruz. Temsillerin derinliği, birçok görsel tanıma görevi için merkezi öneme sahiptir. Sadece son derece derin temsillerimiz sayesinde, COCO nesne algılama veri setinde %28'lik göreceli bir iyileştirme elde ediyoruz. Deep residual network'ler, ILSVRC ve COCO 2015 yarışmalarına yaptığımız başvuruların temelini oluşturuyor; burada ayrıca <strong>ImageNet detection, ImageNet localization, COCO detection ve COCO segmentation görevlerinde de 1. sıraları kazandık.</strong>"
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                <strong>Degradation Problem</strong>: Ek layer'lar hem test hem de eğitim hatasını artırabilir, yani sadece overfitting değil. Bu sezgiye aykırıdır çünkü daha derin ağlar, bazı layer'ları identity function olarak ayarlayarak daha sığ ağları temsil edebilir.
                            </li>
                            <li>
                                <strong>Residual Networks (ResNet)'i tanıtıyor</strong>. Temel fikir, bir veya daha fazla layer'ı atlayan ve böylece layer'ların F(x) = H(x) - x residual function'ını temsil etmesini sağlayan <strong>skip connection'lardır</strong>.
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="neural-machine-translation-by-jointly-learning-to-align-and-translate" data-year="2015" data-tags="deep-learning nlp">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="neural-machine-translation-by-jointly-learning-to-align-and-translate"></i>
                        <h1>Neural Machine Translation by Jointly Learning to Align and Translate</h1>
                        <div class="paper-meta">
                            <span><em>Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio, 2015</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1409.0473">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>
                                "Neural machine translation makine çevirisi için yakın zamanda önerilen bir yaklaşımdır. Geleneksel istatistiksel makine çevirisinin aksine, neural machine translation, çeviri performansını en üst düzeye çıkarmak için birlikte ayarlanabilen tek bir neural network oluşturmayı amaçlar. Neural machine translation için son zamanlarda önerilen modeller genellikle encoder-decoder ailesine aittir ve bir kaynak cümleyi, bir decoder'ın çeviri oluşturduğu sabit uzunluklu bir vektöre kodlayan bir encoder'dan oluşur. Bu makalede, <strong>sabit uzunluklu bir vektör kullanımının bu temel encoder-decoder mimarisinin performansını artırmada bir darboğaz olduğunu tahmin ediyor ve bir modelin, bu parçaları açıkça sert bir segment olarak oluşturmak zorunda kalmadan, bir hedef kelimeyi tahmin etmek için kaynak cümlenin ilgili kısımlarını otomatik olarak (yumuşak) aramasına izin vererek bunu genişletmeyi öneriyoruz</strong>. Bu yeni yaklaşımla, İngilizce-Fransızca çevirisi görevinde mevcut state-of-the-art cümle tabanlı sisteme benzer bir çeviri performansı elde ediyoruz. Ayrıca, nitel analiz, model tarafından bulunan (yumuşak) hizalamaların sezgimizle iyi uyuştuğunu ortaya koyuyor."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                <strong>Attention Mechanism'i tanıtıyor</strong>, makine çevirisi ve diğer birçok sequence-to-sequence görevinde önemli bir atılım. Attention mekanizması, modelin hedef cümledeki her kelimeyi oluştururken kaynak cümlenin farklı bölümlerine odaklanmasına olanak tanıyarak, önceki encoder-decoder mimarilerindeki sabit uzunluklu bağlam vektörlerinin sınırlamalarını ele alır.
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="pointer-networks" data-year="2015" data-tags="deep-learning nlp">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="pointer-networks"></i>
                        <h1>Pointer Networks</h1>
                        <div class="paper-meta">
                            <span><em>Oriol Vinyals, Meire Fortunato, Navdeep Jaitly. 2015</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1506.03134">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <img src="https://mfaulk.github.io/assets/images/ptr_net.png" alt="Pointer Network"/>
                        </p>
                        <p>
                            <em>
                                "Öğeleri bir giriş dizisindeki konumlara karşılık gelen ayrık token'lar olan bir <strong>çıktı dizisinin koşullu olasılığını öğrenmek için yeni bir neural architecture</strong> sunuyoruz. Bu tür problemler, mevcut sequence-to-sequence ve Neural Turing Machines gibi yaklaşımlarla kolayca ele alınamaz, çünkü çıktının her adımındaki hedef sınıfların sayısı, değişken olan girişin uzunluğuna bağlıdır. <strong>Değişken boyutlu dizileri sıralama ve çeşitli kombinatoryal optimizasyon problemleri bu sınıfa aittir</strong>. Modelimiz, değişken boyutlu çıktı sözlükleri problemini yakın zamanda önerilen neural attention mekanizmasını kullanarak çözer. Daha önceki attention girişimlerinden farkı, her decoder adımında bir encoder'ın gizli birimlerini bir bağlam vektörüne karıştırmak için attention kullanmak yerine, <strong>çıktı olarak giriş dizisinin bir üyesini seçmek için bir işaretçi olarak attention kullanmasıdır</strong>. Bu mimariye bir Pointer Net (Ptr-Net) diyoruz. Ptr-Net'lerin, yalnızca eğitim örneklerini kullanarak üç zorlu geometrik probleme - düzlemsel konveks gövdeleri bulma, Delaunay üçgenlemelerini hesaplama ve düzlemsel Gezgin Satıcı Problemi - yaklaşık çözümler öğrenmek için kullanılabileceğini gösteriyoruz. Ptr-Net'ler sadece giriş dikkatli sequence-to-sequence'ı geliştirmekle kalmaz, aynı zamanda değişken boyutlu çıktı sözlüklerine genelleme yapmamıza da olanak tanır. Öğrenilen modellerin, eğitildikleri maksimum uzunlukların ötesine genelleştirdiğini gösteriyoruz. Bu görevlerdeki sonuçlarımızın, ayrık problemler için neural öğrenmenin daha geniş bir şekilde keşfedilmesini teşvik edeceğini umuyoruz."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>Değişken Çıktı Sözlükleri: Çıktının her adımındaki hedef sınıfların sayısı, değişken olan girişin uzunluğuna bağlıdır. Geleneksel sequence-to-sequence modelleri sabit bir çıktı kelime dağarcığı gerektirir.</li>
                            <li>
                                Çıktının her adımında, model <strong>çıktı olarak giriş dizisinin bir üyesini seçmek için girişler üzerinde attention'ı bir işaretçi olarak kullanır</strong>. Bu, modelin değişken boyutlu çıktı sözlüklerine genelleme yapmasına olanak tanır.
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="recurrent-neural-network-regularization" data-year="2015" data-tags="deep-learning nlp">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="recurrent-neural-network-regularization"></i>
                        <h1>Recurrent Neural Network Regularization</h1>
                        <div class="paper-meta">
                            <span><em>Zaremba, Sutskever, Vinyals. 2015</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1409.2329">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>
                                "Uzun Kısa Süreli Bellek (LSTM) birimleri ile Recurrent Neural Networks (RNN'ler) için basit bir regularization tekniği sunuyoruz. Neural networklerini düzenlemek için en başarılı teknik olan dropout, RNN'ler ve LSTM'ler ile iyi çalışmaz. Bu makalede, <strong>dropout'un LSTM'lere nasıl doğru şekilde uygulanacağını gösteriyor</strong> ve bunun çeşitli görevlerde aşırı uyumu önemli ölçüde azalttığını gösteriyoruz. Bu görevler arasında dil modelleme, konuşma tanıma, görüntü başlığı oluşturma ve makine çevirisi yer alır."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>Dropout'u yalnızca RNN'lerin bağlantılarının bir alt kümesine uygular.</li>
                            <li>
                                <em>"Tekrarlayan bağlantılarda dropout kullanmayarak, LSTM değerli hafıza yeteneğinden ödün vermeden dropout düzenlemesinden yararlanabilir."</em>
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="the-unreasonable-effectiveness-of-recurrent-neural-networks" data-year="2015" data-tags="deep-learning nlp">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="the-unreasonable-effectiveness-of-recurrent-neural-networks"></i>
                        <h1>The Unreasonable Effectiveness of Recurrent Neural Networks</h1>
                        <div class="paper-meta">
                            <span><em>Andrej Karpathy, 2015</em></span>
                            <a class="paper-link" target="_blank" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">
                                <i class="fas fa-blog"></i> Blog Post
                            </a>
                        </div>
                        <p>OpenAI'nin kurucu ortaklarından birinin bu blog yazısı, dizi modellemesi için Recurrent Neural Networks (RNN'ler) gücüne harika bir giriştir. RNN'lerin aldatıcı bir şekilde basit olduğunu gösterir ve RNN'lerin yalnızca karakterden karaktere tahminler kullanarak makul metin oluşturma gücünü gösterir. Üretken dil modellerinin birkaç yıl önce nerede olduğunu ve o zamandan beri ne kadar ilerlediğini görmek eğlenceli.</p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                <em>"Vanilla neural networkleri eğitmek fonksiyonlar üzerinde optimizasyonsa, recurrent networkleri eğitmek programlar üzerinde optimizasyondur."</em>
                            </li>
                            <li>Feed-Forward Neural Networks, sabit boyutlu girişler ve sabit boyutlu çıkışlarla sınırlıdır.</li>
                            <li>RNN'ler keyfi uzunluktaki diziler üzerinde çalışır, bu da onları konuşma tanıma, dil modelleme ve makine çevirisi için uygun hale getirir.</li>
                        </ul>
                    </div>

                    <div class="paper-card" id="understanding-lstm-networks" data-year="2015" data-tags="deep-learning nlp">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="understanding-lstm-networks"></i>
                        <h1>Understanding LSTM Networks</h1>
                        <div class="paper-meta">
                            <span><em>Christopher Olah, 2015</em></span>
                            <a class="paper-link" target="_blank" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">
                                <i class="fas fa-blog"></i> Blog Post
                            </a>
                        </div>
                        <p>
                            Anthropic'in kurucu ortaklarından birinin bu blog yazısı, <a target="_blank" href="https://deeplearning.cs.cmu.edu/F23/document/readings/LSTM.pdf">Long Short-Term Memory</a> (LSTM) networks adı verilen önemli bir RNN türüne harika bir giriştir. Adım adım bir şekilde, bir LSTM biriminin farklı "gate"lerinin bilgiyi depolamak ve almak için nasıl birlikte çalıştığını açıklar.
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                LSTM'ler, bilgiyi uzun süre depolayabilen <strong>memory cell adı verilen bir gizli durum</strong> ekleyerek RNN'leri geliştirir. Bu, uzun dizilerden öğrenme yeteneklerini artırır ve onları doğal dil işleme için uygun hale getirir. Buna karşılık, normal RNN'ler, diyelim ki bir cümlede birbirinden uzak olan kelimeler gibi ayrı bilgi parçalarını bağlamakta zorlanır.
                            </li>
                            <li>
                                Bir LSTM birimi, bilginin memory cell'e girişini ve çıkışını düzenleyen bir <strong>memory cell</strong>, bir <strong>input gate</strong>, bir <strong>output gate</strong> ve bir <strong>forget gate</strong> içerir.
                            </li>
                            <li>
                                1997'den bir fikrin (LSTM'ler) 2015'te araştırma faaliyetinin ön saflarına dönmesini görmek ilginçtir. Bu 18 yılda, <a target="_blank" href="https://www.flickr.com/photos/jurvetson/51391518506/">hesaplama gücündeki oldukça dramatik artışlar</a> da dahil olmak üzere pek çok şey oldu.
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin" data-year="2016" data-tags="deep-learning nlp">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin"></i>
                        <h1>Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</h1>
                        <div class="paper-meta">
                            <span><em>Amodei, et al. 2016</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1512.02595">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>"End-to-end deep learning yaklaşımının İngilizce veya Mandarin Çincesi konuşmasını tanımak için kullanılabileceğini gösteriyoruz - birbirinden çok farklı iki dil. El ile tasarlanmış bileşenlerden oluşan tüm pipeline'ları neural network'lerle değiştirdiği için, end-to-end learning gürültülü ortamlar, aksanlar ve farklı diller dahil çeşitli konuşmaları ele almamıza olanak tanır. Yaklaşımımızın anahtarı, daha önce haftalar süren deneylerin şimdi günler içinde çalışmasını sağlayan HPC tekniklerini uygulamamızdır. Bu, üstün mimariler ve algoritmalar belirlemek için daha hızlı iterasyon yapmamıza olanak tanır. Sonuç olarak, birçok durumda, sistemimiz standart veri setlerinde insan çalışanların transkripsiyonu ile rekabet edebilir durumdadır. Son olarak, data center'daki GPU'larla Batch Dispatch adlı bir teknik kullanarak, sistemimizin ölçekli kullanıcılara hizmet verirken düşük gecikme süresiyle çevrimiçi bir ortamda uygun maliyetle devreye alınabileceğini gösteriyoruz."</em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                <strong>End-to-End Learning</strong>
                                el ile tasarlanmış bileşenlerden oluşan tüm pipeline'ları (özellikler, akustik modeller, dil modelleri vb.) neural network'lerle değiştirir. Bu, iki çok farklı dil üzerinde gösterilmiştir: İngilizce ve Mandarin.
                            </li>
                            <li>
                                <strong>Yüksek düzeyde optimize edilmiş eğitim sistemi</strong>
                                8 veya 16 GPU ile.
                            </li>
                            <li>Modeller yaklaşık 100 milyon parametre içerir.</li>
                        </ul>
                    </div>

                    <div class="paper-card" id="identity-mappings-in-deep-residual-networks" data-year="2016" data-tags="deep-learning cv">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="identity-mappings-in-deep-residual-networks"></i>
                        <h1>Identity Mappings in Deep Residual Networks</h1>
                        <div class="paper-meta">
                            <span><em>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 2016</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1603.05027">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>
                                "Deep residual network'ler, etkileyici doğruluk ve güzel yakınsama davranışları gösteren son derece derin mimariler olarak ortaya çıkmıştır. Bu makalede, <strong>residual building block'ların arkasındaki yayılma formülasyonlarını analiz ediyoruz</strong>
                                , bu da <strong>identity mapping'leri skip connection'lar ve ekleme sonrası aktivasyon olarak kullanırken, ileri ve geri sinyallerin bir bloktan diğer herhangi bir bloğa doğrudan yayılabileceğini</strong>
                                 gösteriyor. Bir dizi ablasyon deneyi, bu identity mapping'lerin önemini destekliyor. Bu bizi, eğitimi kolaylaştıran ve genellemeyi iyileştiren yeni bir residual unit önermeye teşvik ediyor. CIFAR-10 (%4.62 hata) ve CIFAR-100 üzerinde 1001 katmanlı bir ResNet ve ImageNet üzerinde 200 katmanlı bir ResNet kullanarak iyileştirilmiş sonuçlar bildiriyoruz."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                Yazarlar, önceki makaleleri olan <a target="_blank" href="#deep-residual-learning-for-image-recognition">Deep Residual Learning for Image Recognition</a>
                                 üzerine inşa ederek, orijinal residual unit'in bazı kısımlarını identity mapping'lere ayarlayarak yeni bir residual unit öneriyorlar. Sonuç, her katman çifti arasında ileri ve geri sinyallere güzel, ekleyici bir yapı sağlayan bir "residual ilişkisi"dir.
                            </li>
                            <li>1001 katmanlı bir ResNet'te yeni residual unit'leri gösteriyor.</li>
                        </ul>
                    </div>

                    <div class="paper-card" id="multi-scale-context-aggregation-by-dilated-convolutions" data-year="2016" data-tags="deep-learning cv">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="multi-scale-context-aggregation-by-dilated-convolutions"></i>
                        <h1>Multi-Scale Context Aggregation by Dilated Convolutions</h1>
                        <div class="paper-meta">
                            <span><em>Fisher Yu, Vladlen Koltun. 2015</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1511.07122">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            Semantic segmentation(anlamsal bölütleme) için state-of-the-art modeller, orijinalinde image classification için tasarlanmış olan convolutional network'lerin adaptasyonlarına dayanmaktadır. Ancak, yoğun tahmin ve görüntü sınıflandırması yapısal olarak farklıdır. Bu çalışmada, <strong>özellikle yoğun tahmin için tasarlanmış yeni bir convolutional network modülü geliştiriyoruz</strong>
                            . Sunulan modül, çözünürlük kaybetmeden çok ölçekli bağlamsal bilgileri sistematik olarak toplamak için <strong>dilated convolution'ları</strong>
                            kullanır. Mimari, <strong>dilated convolution'ların çözünürlük veya kapsam kaybı olmadan alıcı alanın üstel genişlemesini desteklediği</strong>
                            gerçeğine dayanmaktadır. Sunulan bağlam modülünün state-of-the-art semantic segmentation sistemlerinin doğruluğunu artırdığını gösteriyoruz. Ayrıca, image classification ağlarının yoğun tahmine adaptasyonunu inceliyor ve adapte edilen ağı basitleştirmenin doğruluğu artırabileceğini gösteriyoruz.
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>Semantic segmentation görüntüdeki her piksele bir etiket atar. Bu, tüm görüntüye tek bir etiket atamayı amaçlayan image classification'dan farklı bir görevdir. Bununla birlikte, birçok semantic segmentation modeli, image classification için tasarlanmış mimarilere dayanmaktadır.</li>
                            <li>Image classification'dan ödünç alınan teknikleri kritik olarak değerlendirir ve pooling ve subsampling katmanlarının semantic segmentation için iyi bir uyum olmayabileceğini bulur.</li>
                            <li>
                                <strong>Çekirdek elemanları arasında boşluklar olan bir konvolüsyon türü olan dilated convolution'ları</strong>
                                 savunur. Bu, parametre sayısını artırmadan alıcı alanı genişletir.
                            </li>
                        </ul>
                        <p>
                            <img src="https://mfaulk.github.io/assets/images/dilatedconvolution.png" alt="Zeebra"/>
                        </p>
                    </div>

                    <div class="paper-card" id="order-matters-sequence-to-sequence-for-sets" data-year="2016" data-tags="deep-learning nlp">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="order-matters-sequence-to-sequence-for-sets"></i>
                        <h1>Order Matters: Sequence to sequence for sets</h1>
                        <div class="paper-meta">
                            <span><em>Oriol Vinyals, Samy Bengio, Manjunath Kudlur. 2015</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1511.06391">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>
                                "Diziler, recurrent neural networkların yeniden ortaya çıkışı sayesinde supervised learning'de birinci sınıf vatandaşlar haline gelmiştir. Bir dizi gözlemden veya gözleme haritalama gerektiren birçok karmaşık görev, artık dizilerin ortak olasılığını verimli bir şekilde temsil etmek için chain rule kullanan sequence-to-sequence (seq2seq) çerçevesiyle formüle edilebilir. Ancak, birçok durumda, değişken boyutlu girdiler ve/veya çıktılar doğal olarak diziler şeklinde ifade edilemeyebilir. Örneğin, görevi onları sıralamak olan bir modele bir dizi sayıyı nasıl gireceğimiz açık değildir; benzer şekilde, çıktılar random variable'lara karşılık geldiğinde ve görev bunların bilinmeyen joint probability'sini modellemek olduğunda, çıktıları nasıl düzenleyeceğimizi bilmiyoruz. Bu makalede, öncelikle çeşitli örnekler kullanarak, altta yatan bir modeli öğrenirken girdi ve/veya çıktı verilerini düzenlediğimiz sıranın önemli ölçüde önemli olduğunu gösteriyoruz. Ardından, <strong>seq2seq çerçevesinin ötesine geçen ve girdi setlerini ilkeli bir şekilde ele alan bir uzantıyı</strong> tartışıyoruz. Ayrıca, eğitim sırasında olası sıralar üzerinde arama yaparak, çıktı kümelerinin yapı eksikliğiyle başa çıkan bir loss öneriyoruz. Sıralamaya ilişkin iddialarımızın ampirik kanıtlarını ve seq2seq çerçevesindeki değişiklikleri, benchmark language modeling ve parsing görevlerinin yanı sıra iki yapay görev üzerinde - sayıları sıralama ve bilinmeyen graphical modellerin joint probability'sini tahmin etme - gösteriyoruz."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>Seq2Seq modelleri doğası gereği sıraya duyarlıdır, ancak sıralama gibi görevler doğası gereği sıraya duyarlı değildir.</li>
                            <li>
                                Read-Process-Write, girdinin <strong>permutation-invariant embedding'ini</strong> oluşturur. Bu, bir LSTM Pointer Network'e beslenir.
                            </li>
                            <li>Eğitim sırasında olası sıralar üzerinde arama yaparak sırasız çıktıları ele alır. Bu biraz garip hissettiriyor...</li>
                        </ul>
                    </div>

                    <div class="paper-card" id="variational-lossy-autoencoder" data-year="2016" data-tags="deep-learning cv">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="variational-lossy-autoencoder"></i>
                        <h1>Variational Lossy Autoencoder</h1>
                        <div class="paper-meta">
                            <span><em>Chen, et al. 2016</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1611.02731">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>
                                "Representation learning, gözlemlenen verilerin belirli yönlerini, sınıflandırma gibi downstream görevlere uygun bir öğrenilmiş gösterimde açığa çıkarmayı amaçlar. Örneğin, 2D görüntüler için iyi bir gösterim, yalnızca global yapıyı tanımlayan ve detaylı doku hakkındaki bilgileri atan bir gösterim olabilir. Bu makalede, <strong>Variational Autoencoder (VAE) ile RNN, MADE ve PixelRNN/CNN gibi neural autoregressive modelleri birleştirerek</strong> bu tür global gösterimleri öğrenmek için basit ama ilkeli bir yöntem sunuyoruz. Önerdiğimiz VAE modeli, global latent code'un ne öğrenebileceği üzerinde kontrol sahibi olmamıza izin verir ve mimariye göre tasarlayarak, global latent code'un 2D görüntülerdeki doku gibi ilgisiz bilgileri atmasını zorlayabiliriz, böylece VAE verileri yalnızca kayıplı bir şekilde "otomatik kodlar". Ayrıca, hem prior distribution p(z) hem de decoding distribution p(x|z) olarak autoregressive modelleri kullanarak, VAE'lerin generative modeling performansını büyük ölçüde iyileştirebilir, MNIST, OMNIGLOT ve Caltech-101 Silhouettes density estimation görevlerinde yeni state-of-the-art sonuçlar elde edebiliriz."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>TODO</li>
                        </ul>
                    </div>

                    <div class="paper-card" id="a-simple-neural-network-module-for-relational-reasoning" data-year="2017" data-tags="deep-learning cv">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="a-simple-neural-network-module-for-relational-reasoning"></i>
                        <h1>A Simple Neural Network Module for Relational Reasoning</h1>
                        <div class="paper-meta">
                            <span><em>Santoro, et al. 2017</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1706.01427">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <img src="https://mfaulk.github.io/assets/images/rn_fig2.png" alt="Relational Reasoning"/>
                        </p>
                        <p>
                            "
                            <em>
                                Relational reasoning, genel olarak zeki davranışın merkezi bir bileşenidir, ancak neural networkler için öğrenmesi zor olduğu kanıtlanmıştır. Bu makalede, <strong>temelde relational reasoning'e bağlı olan problemleri çözmek için Relation Networks (RN)'i basit bir tak-çalıştır modülü olarak nasıl kullanacağımızı</strong> açıklıyoruz. RN ile güçlendirilmiş ağları üç görevde test ettik: CLEVR adında zorlu bir veri seti kullanarak visual question answering, ki burada state-of-the-art, insan üstü performansa ulaşıyoruz; bAbI görev seti kullanarak metin tabanlı soru cevaplama; ve dinamik fiziksel sistemler hakkında karmaşık akıl yürütme. Daha sonra, Sort-of-CLEVR adlı düzenlenmiş bir veri seti kullanarak, <strong>güçlü convolutional networkların ilişkisel soruları çözmek için genel bir kapasiteye sahip olmadığını, ancak RN'lerle güçlendirildiklerinde bu kapasiteyi kazanabileceklerini</strong> gösteriyoruz. Çalışmamız, <strong>bir RN modülü ile donatılmış derin öğrenme mimarisinin, varlıkları ve ilişkilerini örtük olarak nasıl keşfedebileceğini ve bunlar hakkında nasıl akıl yürütmeyi öğrenebileceğini</strong> göstermektedir.
                            </em>
                            "
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                <strong>Relational reasoning</strong>, farklı varlıklar veya bilgi parçaları arasındaki ilişkileri anlama, çıkarım yapma ve manipüle etme yeteneğidir.
                            </li>
                            <li>
                                <a target="_blank" href="https://cs.stanford.edu/people/jcjohns/clevr/">CLEVR</a> visual reasoning görevinde insan üstü performans elde eder.
                            </li>
                            <li>RN'ler, bir NN'in fonksiyonel formunu, convolutional katmanların translational invariance yakalaması veya recurrent katmanların sequential dependencies yakalaması gibi, relational reasoning'in ortak özelliklerini yakalayacak şekilde kısıtlar.</li>
                            <li>RN'ler bir nesne kümesi üzerinde çalışır ve aralarındaki (ikili) ilişkileri öğrenir.</li>
                        </ul>
                    </div>

                    <div class="paper-card" id="attention-is-all-you-need" data-year="2017" data-tags="deep-learning nlp">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="attention-is-all-you-need"></i>
                        <h1>Attention is All You Need</h1>
                        <div class="paper-meta">
                            <span><em>Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin. 2017</em></span>
                        </div>
                        <p>
                            <a target="_blank" href="https://nlp.seas.harvard.edu/annotated-transformer/">(Faydalı Açıklamalı Makale)</a>
                            <a target="_blank" href="https://arxiv.org/pdf/1706.03762">(Orijinal Makale)</a>
                        </p>
                        <p>
                            "Baskın sequence transduction modelleri, bir encoder ve decoder içeren karmaşık recurrent veya convolutional neural networkler'e dayanmaktadır. En iyi performans gösteren modeller ayrıca encoder ve decoder'ı bir attention mechanism aracılığıyla birbirine bağlar. <strong>Recurrence ve convolution'dan tamamen vazgeçerek, yalnızca attention mechanism'lerine dayalı yeni ve basit bir network mimarisi olan Transformer'ı öneriyoruz.</strong> İki makine çevirisi görevi üzerindeki deneyler gösteriyor ki..."
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                <strong>Transformer mimarisi</strong>, RNN'lerin recurrent (ardışık) bağlantılarını ortadan kaldırır. Bu, GPU'larla verimli ve paralel eğitime olanak tanır.
                            </li>
                            <li>
                                <strong>Self-attention mekanizması</strong>, modelin bir cümledeki farklı kelimelerin önemini, konumlarından bağımsız olarak ağırlıklandırmasını sağlar. Bu, uzun menzilli bağımlılıkları ve ilişkileri RNN'lerden daha etkili bir şekilde yakalamaya olanak tanır.
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="kolmogorov-complexity-and-algorithmic-randomness" data-year="2017" data-tags="theory">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="kolmogorov-complexity-and-algorithmic-randomness"></i>
                        <h1>Kolmogorov Complexity and Algorithmic Randomness</h1>
                        <div class="paper-meta">
                            <span><em>"Sıfır ve birlerden oluşan bir diziye baktığımızda, çoğu zaman bunun rastgele olmadığını, yani adil bir para atmadan elde edilmiş olamayacak kadar düzensiz olmadığını hissederiz. Neden? Cevap, algorithmic information theory tarafından sağlanır: çünkü dizi sıkıştırılabilir, yani düşük complexity'ye sahiptir veya eşdeğer olarak kısa bir program tarafından üretilebilir. Solomonoff, Kolmogorov, Chaitin, Levin ve diğerleri tarafından geliştirilen bu fikir, şimdi algorithmic information theory'nin başlangıç noktasıdır. Bu kitabın ilk kısmı, complexity ve randomness'ın temel kavramlarının ders kitabı tarzında bir açıklamasıdır; ikinci kısım, 1980'lerde Kolmogorov'un kendisi tarafından başlatılan Moskova'daki 'Kolmogorov seminerine' katılanlar ve meslektaşları tarafından yapılan bazı son çalışmaları kapsamaktadır."</em></span>
                            <a class="paper-link" target="_blank" href="https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            Bu oldukça kapsamlı bir kitap ve ben sadece göz gezdirdim. Bunun yerine <a target="_blank" href="https://link.springer.com/content/pdf/10.1007/978-3-030-11298-1.pd">An Introduction to Kolmogorov Complexity and Its Applications</a>'a bakmanızı öneririm.
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>Kolmogorov Complexity, bir nesnedeki karmaşıklığın veya bilgi miktarının evrensel bir tanımıdır.</li>
                            <li>Bu, bilinen alternatifler (semboller) listesinden bir nesneyi seçmek için iletilmesi gereken bilgi miktarı olan Shannon's Entropy'den farklıdır.</li>
                        </ul>
                    </div>

                    <div class="paper-card" id="neural-message-passing-for-quantum-chemistry" data-year="2017" data-tags="deep-learning">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="neural-message-passing-for-quantum-chemistry"></i>
                        <h1>Neural Message Passing for Quantum Chemistry</h1>
                        <div class="paper-meta">
                            <span><em>Gilmer, et al. 2017</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1704.01212">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <img src="https://mfaulk.github.io/assets/images/mpnn_fig_1.png" alt="Message Passing Neural Networks"/>
                        </p>
                        <p>
                            <em>
                                "Moleküller üzerinde supervised learning, kimya, ilaç keşfi ve malzeme bilimi için inanılmaz bir potansiyele sahiptir. Neyse ki, moleküler simetrilere karşı değişmez olan birkaç umut verici ve yakından ilişkili neural network modeli zaten literatürde tanımlanmıştır. <strong>Bu modeller, tüm giriş grafiğinin bir fonksiyonunu hesaplamak için bir message passing algoritması ve aggregation prosedürü öğrenir</strong>. Bu noktada, bir sonraki adım, bu genel yaklaşımın özellikle etkili bir varyantını bulmak ve ya onları çözene kadar ya da yaklaşımın sınırlarına ulaşana kadar kimyasal tahmin benchmark'larına uygulamaktır. Bu makalede, mevcut modelleri Message Passing Neural Networks (MPNNs) olarak adlandırdığımız tek bir ortak çerçevede yeniden formüle ediyoruz ve bu çerçeve içinde ek yeni varyasyonları keşfediyoruz. MPNN'leri kullanarak, önemli bir moleküler özellik tahmini benchmark'ında state of the art sonuçlar gösteriyoruz; bu sonuçlar, gelecekteki çalışmaların daha büyük moleküller veya daha doğru ground truth etiketlere sahip veri setlerine odaklanması gerektiğine inandığımız kadar güçlüdür."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                <strong>Message Passing Neural Networks (MPNNs)</strong>, graf tabanlı veriler için çeşitli modellerin ortak özelliklerini soyutlar. Düğümler, komşularına yinelemeli olarak mesajlar gönderir, komşular bu mesajları toplar ve kendi durumlarını günceller. Son olarak, bir readout fonksiyonu, son düğüm durumlarını grafın global durumuna eşler.
                            </li>
                            <li>
                                Grafın sırasına değişmez bir readout üretmek için <a target="_blank" href="https://arxiv.org/abs/1511.06391">set2set</a> kullanır.
                            </li>
                        </ul>
                    </div>

                    <div class="paper-card" id="relational-recurrent-neural-networks" data-year="2018" data-tags="deep-learning nlp">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="relational-recurrent-neural-networks"></i>
                        <h1>Relational Recurrent Neural Networks</h1>
                        <div class="paper-meta">
                            <span><em>Santoro, Adam, et al. 2018</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1806.01822">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>
                                "Hafıza tabanlı neural networkler, bilgiyi uzun süre hatırlama yeteneklerini kullanarak zamansal verileri modellerler. Ancak, hatırladıkları bilgilerle karmaşık ilişkisel muhakeme yapma yeteneğine sahip olup olmadıkları belirsizdir. Burada, öncelikle standart bellek mimarilerinin, varlıkların nasıl bağlandığını anlamayı içeren görevlerde - yani ilişkisel muhakeme içeren görevlerde - zorlanabileceğine dair sezgilerimizi doğruluyoruz. Daha sonra, <strong>hafızaların etkileşimini sağlamak için multi-head dot product attention kullanan yeni bir hafıza modülü - Relational Memory Core (RMC) - kullanarak</strong> bu eksiklikleri iyileştiriyoruz. Son olarak, RMC'yi sıralı bilgiler arasında daha yetenekli ilişkisel muhakemeden faydalanabilecek bir dizi görevde test ediyoruz ve RL alanlarında (örn. Mini PacMan), program değerlendirmesinde ve dil modellemesinde büyük kazanımlar gösteriyoruz, WikiText-103, Project Gutenberg ve GigaWord veri setlerinde state-of-the-art sonuçlara ulaşıyoruz."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>"Bir modelin bilgiyi bölümleyebileceği ve bölümlendirilmiş bilgi arasında etkileşimleri hesaplamayı öğrenebileceği bir mimari omurga."</li>
                            <li>Relational Memory Core (RMC), satır bazlı hafızalar matrisi tutar. Güncellemeler, önceki hafızalar ve giriş üzerindeki attention aracılığıyla yapılır.</li>
                            <li>Hafıza matrisi, bir 2D-LSTM'deki hücre durumları matrisi olarak görülebilir.</li>
                        </ul>
                    </div>

                    <div class="paper-card" id="gpipe-efficient-training-of-giant-neural-networks-using-pipeline-parallelism" data-year="2019" data-tags="deep-learning">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="gpipe-efficient-training-of-giant-neural-networks-using-pipeline-parallelism"></i>
                        <h1>GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</h1>
                        <div class="paper-meta">
                            <span><em>Hung, et al. 2018</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/1811.06965">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                            <a class="paper-link" target="_blank" href="https://research.google/blog/introducing-gpipe-an-open-source-library-for-efficiently-training-large-scale-neural-network-models/">
                                <i class="fas fa-blog"></i> Blog
                            </a>
                        </div>
                        <p>
                            <img src="https://mfaulk.github.io/assets/images/gpipe.png" alt="Micro-batch pipelining"/>
                        </p>
                        <p>
                            <em>
                                "Deep neural network kapasitesini artırmanın, çeşitli makine öğrenimi görevleri için model kalitesini iyileştirmenin etkili bir yaklaşımı olduğu bilinmektedir. Birçok durumda, model kapasitesini tek bir accelerator'ın bellek sınırının ötesine artırmak, özel algoritmalar veya altyapı geliştirmeyi gerektirmiştir. Bu çözümler genellikle mimariye özgüdür ve diğer görevlere aktarılamaz. Verimli ve görevden bağımsız model paralelliği ihtiyacını karşılamak için, <strong>katmanların bir dizisi olarak ifade edilebilen herhangi bir ağı ölçeklendirmeye olanak tanıyan bir pipeline parallelism kütüphanesi olan GPipe'ı tanıtıyoruz</strong>. GPipe, farklı katman alt dizilerini ayrı accelerator'larda pipeline haline getirerek, çeşitli farklı ağları devasa boyutlara verimli bir şekilde ölçeklendirme esnekliği sağlar. Üstelik GPipe, bir model birden fazla accelerator arasında bölündüğünde neredeyse doğrusal hızlanma sağlayan yeni bir batch-splitting pipelining algoritması kullanır. GPipe'ın avantajlarını, farklı ağ mimarilerine sahip iki farklı görevde büyük ölçekli neural network'leri eğiterek gösteriyoruz: (i) Image Classification: 557 milyon parametreli bir AmoebaNet modeli eğitiyoruz ve ImageNet-2012'de %84.4'lük bir top-1 doğruluk elde ediyoruz, (ii) Çok Dilli Neural Machine Translation: <strong>100'den fazla dili kapsayan bir corpus üzerinde tek bir 6 milyar parametreli, 128 katmanlı Transformer modeli eğitiyoruz ve tüm ikili dillimodellere göre daha iyi kalite elde ediyoruz</strong>."
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>Model boyutunu artırmak genellikle model performansını iyileştirir, ancak model büyümesi donanım büyümesini geçmektedir.</li>
                            <li>
                                GPipe, birden fazla accelerator üzerinde eğitim için synchronous stochastic gradient descent ve <strong>pipeline parallelism</strong> kullanan dağıtılmış bir makine öğrenimi <a target="_blank" href="https://github.com/tensorflow/lingvo/blob/master/lingvo/core/gpipe.py">kütüphanesidir</a>.
                            </li>
                            <li>İleri geçiş: mini-batch'ler micro-batch'lere bölünür ve accelerator'lar arasında pipeline'lanır. Geri geçiş: gradientler micro-batch'ler arasında biriktirilir.</li>
                        </ul>
                    </div>

                    <div class="paper-card" id="scaling-laws-for-neural-language-models" data-year="2020" data-tags="deep-learning nlp">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="scaling-laws-for-neural-language-models"></i>
                        <h1>Scaling Laws for Neural Language Models</h1>
                        <div class="paper-meta">
                            <span><em>Kaplan, et al. 2020</em></span>
                            <a class="paper-link" target="_blank" href="https://arxiv.org/pdf/2001.08361">
                                <i class="fas fa-file-pdf"></i> PDF
                            </a>
                        </div>
                        <p>
                            <em>
                                "Neural language model performansı için ampirik scaling law'ları inceliyoruz. <strong>Kayıp (loss), model boyutu, veri seti boyutu ve eğitim için kullanılan compute miktarı ile bir power-law olarak ölçeklenir</strong>, bazı trendler yedi büyüklük mertebesinden fazlasını kapsar. Network genişliği veya derinliği gibi diğer mimari detayların geniş bir aralıkta minimal etkileri vardır. Basit denklemler, overfitting'in model/veri seti boyutuna bağlılığını ve eğitim hızının model boyutuna bağlılığını yönetir. Bu ilişkiler, <strong>sabit bir compute bütçesinin optimal tahsisini</strong> belirlememize olanak tanır. Daha büyük modeller önemli ölçüde daha sample-efficient'tır, öyle ki <strong>optimum compute-efficient eğitim, çok büyük modelleri nispeten mütevazı miktarda veri üzerinde eğitmeyi ve yakınsama öncesinde önemli ölçüde durdurmayı içerir.</strong>"
                            </em>
                        </p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <ul>
                            <li>
                                <strong>"Language modeling performansı, model boyutunu, veriyi ve compute'u uygun şekilde ölçeklendirdiğimizde yumuşak ve tahmin edilebilir bir şekilde gelişir."</strong>
                            </li>
                            <li>
                                Language model (Transformer) <strong>performansı en güçlü şekilde ölçeğe bağlıdır</strong>: model boyutu, veri seti boyutu ve compute kaynakları.
                            </li>
                            <li>
                                Performans, her ölçek faktörüyle yumuşak bir <a target="_blank" href="https://en.wikipedia.org/wiki/Power_law">power-law</a> ilişkisine sahiptir (bu, artan ölçekle azalan marjinal getiriler anlamına gelir).
                            </li>
                            <li>Performans, "birçok mimari ve optimizasyon hiper-parametresine çok zayıf bağlıdır."</li>
                            <li>
                                "Sonuçlarımız, daha büyük modellerin performanslarının daha iyi olmaya devam edeceğini ve ayrıca daha önce takdir edilenden çok daha sample-efficient olacağını güçlü bir şekilde göstermektedir. <strong>Büyük veri yerine büyük modeller daha önemli olabilir</strong>."
                            </li>
                        </ul>
                        <p>
                            <strong>
                                UYARI: <a target="_blank" href="https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models">Herkes bu scaling law'lar konusunda hemfikir değil.</a>
                            </strong>
                        </p>
                    </div>

                    <div class="paper-card" id="cs231n-convolutional-neural-networks-for-visual-recognition" data-year="2024" data-tags="deep-learning cv">
                        <i class="fas fa-bookmark bookmark-icon" data-paper-id="cs231n-convolutional-neural-networks-for-visual-recognition"></i>
                        <h1>CS231n Convolutional Neural Networks for Visual Recognition</h1>
                        <div class="paper-meta">
                            <a class="paper-link" target="_blank" href="https://cs231n.github.io/">
                                <i class="fas fa-book"></i> 2024 Ders Notları
                            </a>
                        </div>
                        <p>"Bu kurs, özellikle image classification olmak üzere, bu görevler için end-to-end modeller öğrenmeye odaklanarak deep learning mimarilerinin detaylarına derinlemesine bir bakıştır."</p>
                        <div class="insight-header">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Çıkarımlar</strong>
                        </div>
                        <p>Ders notları, neural network'lerin temellerinin mükemmel bir açıklamasını içerir, örneğin:</p>
                        <ul>
                            <li>
                                <a target="_blank" href="https://cs231n.github.io/neural-networks-1/">Nöron modeli ve NN mimarileri</a>
                            </li>
                            <li>
                                <a target="_blank" href="https://cs231n.github.io/optimization-2/">Backpropagation</a>
                            </li>
                            <li>
                                <a target="_blank" href="https://cs231n.github.io/optimization-1/">Öğrenme için Gradient Descent</a>
                            </li>
                        </ul>
                    </div>

                    <!-- Additional paper cards would follow the same pattern -->
                    <!-- ... existing paper cards ... -->

                </div>
            </article>
        </div>
    </main>

    <button id="scrollTopBtn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>

    <!-- Theme selector button and panel -->
    <div class="theme-palette-btn" id="themePaletteBtn">
        <i class="fas fa-palette"></i>
    </div>

    <div class="theme-selector" id="themeSelector">
        <div class="theme-option theme-dark active" data-theme="dark" title="Dark Theme"></div>
        <div class="theme-option theme-light" data-theme="light" title="Light Theme"></div>
        <div class="theme-option theme-cyberpunk" data-theme="cyberpunk" title="Cyberpunk Theme"></div>
        <div class="theme-option theme-nature" data-theme="nature" title="Nature Theme"></div>
    </div>

    <script>
        // Dark/Light Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const body = document.body;
        
        // Check if user has set a theme preference
        const savedTheme = localStorage.getItem('theme');
        if (savedTheme) {
            body.setAttribute('data-theme', savedTheme);
            // Update the active theme option
            document.querySelectorAll('.theme-option').forEach(option => {
                option.classList.remove('active');
                if (option.dataset.theme === savedTheme) {
                    option.classList.add('active');
                }
            });
        }
        
        themeToggle.addEventListener('click', () => {
            const currentTheme = body.getAttribute('data-theme') || 'dark';
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            body.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
            
            // Update the active theme option
            document.querySelectorAll('.theme-option').forEach(option => {
                option.classList.remove('active');
                if (option.dataset.theme === newTheme) {
                    option.classList.add('active');
                }
            });
        });

        // Multiple Theme Options
        const themePaletteBtn = document.getElementById('themePaletteBtn');
        const themeSelector = document.getElementById('themeSelector');
        const themeOptions = document.querySelectorAll('.theme-option');
        
        themePaletteBtn.addEventListener('click', () => {
            themeSelector.classList.toggle('active');
            themePaletteBtn.classList.toggle('active');
        });
        
        themeOptions.forEach(option => {
            option.addEventListener('click', () => {
                const theme = option.dataset.theme;
                body.setAttribute('data-theme', theme);
                localStorage.setItem('theme', theme);
                
                themeOptions.forEach(o => o.classList.remove('active'));
                option.classList.add('active');
            });
        });
        
        // Close theme selector when clicking outside
        document.addEventListener('click', (e) => {
            if (!themeSelector.contains(e.target) && e.target !== themePaletteBtn) {
                themeSelector.classList.remove('active');
                themePaletteBtn.classList.remove('active');
            }
        });

        // TOC Toggle
        const tocToggle = document.getElementById('tocToggle');
        const tocContainer = document.getElementById('tocContainer');
        const overlay = document.getElementById('overlay');

        tocToggle.addEventListener('click', () => {
            tocContainer.classList.toggle('active');
            overlay.classList.toggle('active');
        });

        overlay.addEventListener('click', () => {
            tocContainer.classList.remove('active');
            overlay.classList.remove('active');
        });

        // Properly Generate Table of Contents from paper cards
        function generateTableOfContents() {
            const paperCards = document.querySelectorAll('.paper-card');
            const tocList = document.getElementById('tocList');
            tocList.innerHTML = ''; // Clear existing items
            
            paperCards.forEach(card => {
                const headingElement = card.querySelector('h1');
                if (!headingElement) return;
                
                const paperTitle = headingElement.textContent;
                const paperId = card.id;
                
                // Extract year from dataset or provide a default
                let paperYear = card.getAttribute('data-year');
                if (!paperYear) {
                    // Try to extract year from the title or set default
                    const yearMatch = paperTitle.match(/\b(19|20)\d{2}\b/);
                    paperYear = yearMatch ? yearMatch[0] : "N/A";
                    
                    // Add year to the card element if it wasn't there
                    card.setAttribute('data-year', paperYear);
                }
                
                const li = document.createElement('li');
                li.dataset.paperId = paperId;
                
                const a = document.createElement('a');
                a.href = `#${paperId}`;
                a.innerHTML = `<span class="paper-year">${paperYear}</span>${paperTitle}`;
                
                a.addEventListener('click', () => {
                    tocContainer.classList.remove('active');
                    overlay.classList.remove('active');
                });
                
                li.appendChild(a);
                tocList.appendChild(li);
            });
        }

        // Reading Progress Bar
        function updateReadingProgress() {
            const winScroll = document.body.scrollTop || document.documentElement.scrollTop;
            const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
            const scrolled = (winScroll / height) * 100;
            document.getElementById("readingProgress").style.width = scrolled + "%";
        }

        window.addEventListener('scroll', updateReadingProgress);

        // Search functionality enhanced
        function setupSearch(inputId, listId, itemSelector) {
            const searchInput = document.getElementById(inputId);
            const items = document.getElementById(listId).querySelectorAll(itemSelector);
            
            searchInput.addEventListener('input', () => {
                const searchTerm = searchInput.value.toLowerCase();
                
                items.forEach(item => {
                    const text = item.textContent.toLowerCase();
                    if (text.includes(searchTerm)) {
                        item.style.display = '';
                        
                        // Highlight matching text
                        if (inputId === 'mainSearchInput') {
                            const link = item.querySelector('a');
                            if (link) {
                                const originalText = link.textContent;
                                if (searchTerm.length > 0) {
                                    const regex = new RegExp(`(${searchTerm})`, 'gi');
                                    link.innerHTML = originalText.replace(regex, '<mark>$1</mark>');
                                } else {
                                    link.textContent = originalText;
                                }
                            }
                        }
                    } else {
                        item.style.display = 'none';
                    }
                });
            });
        }

        // Category/Tag Filtering
        function setupTagFiltering() {
            const tagButtons = document.querySelectorAll('.tag');
            const paperItems = document.querySelectorAll('.paper-list li');
            const paperCards = document.querySelectorAll('.paper-card');
            
            tagButtons.forEach(tag => {
                tag.addEventListener('click', () => {
                    const selectedTag = tag.dataset.tag;
                    
                    // Update active tag styling
                    tagButtons.forEach(t => t.classList.remove('active'));
                    tag.classList.add('active');
                    
                    // Filter paper list items
                    paperItems.forEach(item => {
                        if (selectedTag === 'all') {
                            item.style.display = '';
                        } else if (selectedTag === 'bookmarked') {
                            const paperId = item.querySelector('a').getAttribute('href').substring(1);
                            const isBookmarked = localStorage.getItem(`bookmark-${paperId}`) === 'true';
                            item.style.display = isBookmarked ? '' : 'none';
                        } else {
                            const tags = item.dataset.tags || '';
                            item.style.display = tags.includes(selectedTag) ? '' : 'none';
                        }
                    });
                    
                    // Filter paper cards
                    paperCards.forEach(card => {
                        if (selectedTag === 'all') {
                            card.style.display = '';
                        } else if (selectedTag === 'bookmarked') {
                            const isBookmarked = localStorage.getItem(`bookmark-${card.id}`) === 'true';
                            card.style.display = isBookmarked ? '' : 'none';
                        } else {
                            const tags = card.dataset.tags || '';
                            card.style.display = tags.includes(selectedTag) ? '' : 'none';
                        }
                    });
                });
            });
        }

        // Bookmarking functionality
        function setupBookmarking() {
            const paperCards = document.querySelectorAll('.paper-card');
            
            // Add bookmark icons to any cards missing them
            paperCards.forEach(card => {
                const paperId = card.id;
                let icon = card.querySelector('.bookmark-icon');
                
                if (!icon) {
                    icon = document.createElement('i');
                    icon.className = 'fas fa-bookmark bookmark-icon';
                    icon.dataset.paperId = paperId;
                    card.insertBefore(icon, card.firstChild);
                } else if (!icon.dataset.paperId) {
                    icon.dataset.paperId = paperId;
                }
                
                // Initialize from localStorage
                const isBookmarked = localStorage.getItem(`bookmark-${paperId}`) === 'true';
                if (isBookmarked) {
                    icon.classList.add('active');
                    card.classList.add('bookmarked');
                }
                
                // Add click event listener
                icon.addEventListener('click', (e) => {
                    e.stopPropagation(); // Prevent event bubbling
                    const isCurrentlyBookmarked = icon.classList.contains('active');
                    
                    if (isCurrentlyBookmarked) {
                        // Remove bookmark
                        icon.classList.remove('active');
                        card.classList.remove('bookmarked');
                        localStorage.setItem(`bookmark-${paperId}`, 'false');
                    } else {
                        // Add bookmark
                        icon.classList.add('active');
                        card.classList.add('bookmarked');
                        localStorage.setItem(`bookmark-${paperId}`, 'true');
                        
                        // Show a cool animation effect
                        card.animate([
                            { transform: 'scale(1)' },
                            { transform: 'scale(1.05)', boxShadow: '0 15px 35px rgba(0, 0, 0, 0.3)' },
                            { transform: 'scale(1)' }
                        ], {
                            duration: 600,
                            easing: 'cubic-bezier(0.34, 1.56, 0.64, 1)'
                        });
                    }
                });
            });
        }

        // Highlight current section in TOC
        function highlightCurrentSection() {
            const paperCards = document.querySelectorAll('.paper-card');
            const tocItems = document.querySelectorAll('.toc-list li');
            
            window.addEventListener('scroll', () => {
                let currentSectionId = '';
                const scrollPosition = window.scrollY + 100;
                
                paperCards.forEach(card => {
                    const offsetTop = card.offsetTop;
                    const height = card.offsetHeight;
                    
                    if (scrollPosition >= offsetTop && scrollPosition < offsetTop + height) {
                        currentSectionId = card.id;
                    }
                });
                
                tocItems.forEach(item => {
                    item.classList.remove('active');
                    if (item.dataset.paperId === currentSectionId) {
                        item.classList.add('active');
                    }
                });
            });
        }

        // Scroll to Top Button
        const scrollTopBtn = document.getElementById('scrollTopBtn');

        window.addEventListener('scroll', () => {
            if (document.body.scrollTop > 300 || document.documentElement.scrollTop > 300) {
                scrollTopBtn.style.display = 'flex';
            } else {
                scrollTopBtn.style.display = 'none';
            }
        });

        scrollTopBtn.addEventListener('click', () => {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Smooth scroll for all anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                
                const targetId = this.getAttribute('href');
                if (targetId === '#') return;
                
                const targetElement = document.querySelector(targetId);
                if (!targetElement) return;
                
                window.scrollTo({
                    top: targetElement.offsetTop - 70,
                    behavior: 'smooth'
                });
                
                // Update URL without scrolling
                history.pushState(null, null, targetId);
            });
        });

        // Add animations for better UI experience
        function addUIAnimations() {
            // Animate paper cards on scroll
            const paperCards = document.querySelectorAll('.paper-card');
            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.style.opacity = '1';
                        entry.target.style.transform = 'translateY(0)';
                    }
                });
            }, { threshold: 0.1 });
            
            paperCards.forEach(card => {
                card.style.opacity = '0';
                card.style.transform = 'translateY(20px)';
                card.style.transition = 'opacity 0.5s ease, transform 0.5s ease';
                observer.observe(card);
            });
        }

        // Add a fancy animation effect for the whole page during load
        function addPageLoadAnimation() {
            const mainContent = document.querySelector('.main-content');
            const header = document.querySelector('.header');
            
            mainContent.style.opacity = '0';
            mainContent.style.transform = 'translateY(20px)';
            
            header.style.opacity = '0';
            header.style.transform = 'translateY(-20px)';
            
            setTimeout(() => {
                header.style.transition = 'opacity 0.8s ease, transform 0.8s ease';
                header.style.opacity = '1';
                header.style.transform = 'translateY(0)';
                
                setTimeout(() => {
                    mainContent.style.transition = 'opacity 1s ease, transform 1s ease';
                    mainContent.style.opacity = '1';
                    mainContent.style.transform = 'translateY(0)';
                }, 300);
            }, 100);
        }
        
        // Initialize all features when the DOM is loaded
        document.addEventListener('DOMContentLoaded', () => {
            addPageLoadAnimation();
            generateTableOfContents();
            setupBookmarking(); // This will add missing bookmark icons
            setupSearch('searchInput', 'tocList', 'li');
            setupSearch('mainSearchInput', 'paperList', 'li');
            setupTagFiltering();
            highlightCurrentSection();
            addUIAnimations();
            updateReadingProgress();
        });
    </script>
</body>
</html>
